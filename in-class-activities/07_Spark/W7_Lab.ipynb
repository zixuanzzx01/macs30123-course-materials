{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MACS 30123 Lab Session: Working with EMR Clusters/Spark\n",
    "*Week 7*\n",
    "\n",
    "**Edited by Ethan Kozlowski, Nalin Bhatt, Max Zhu (developed from Adam Wu, Wonje Yun)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create S3 bucket to store our files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T22:57:55.401001Z",
     "start_time": "2024-05-06T22:57:55.391513Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T22:57:57.308009Z",
     "start_time": "2024-05-06T22:57:56.903779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize boto3 handler\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Create a new bucket to store your files\n",
    "BUCKETNAME = 'ethankoz-bucket'\n",
    "s3.create_bucket(Bucket=BUCKETNAME)\n",
    "\n",
    "# This is what we will use to interface with the specific bucket\n",
    "bucket = s3.Bucket( BUCKETNAME )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T22:58:08.783183Z",
     "start_time": "2024-05-06T22:58:08.543064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Upload your .py file to S3\n",
    "\n",
    "FILENAME = 'mystuff/myfile.py'\n",
    "with open('mystuff/myfile.py', 'rb') as myfile:\n",
    "    bucket.put_object(Key=FILENAME, Body=myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching EMR Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next launch EMR Cluster in Terminal/bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T22:24:59.258263Z",
     "start_time": "2024-05-06T22:24:56.359573Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "aws emr create-cluster \\\n",
    "    --name \"Spark Cluster\" \\\n",
    "    --release-label \"emr-6.2.0\" \\\n",
    "    --applications Name=Hadoop Name=Hive Name=JupyterEnterpriseGateway Name=JupyterHub Name=Livy Name=Pig Name=Spark Name=Tez \\\n",
    "    --instance-type m5.xlarge \\\n",
    "    --instance-count 3 \\\n",
    "    --use-default-roles \\\n",
    "    --region us-east-1 \\\n",
    "    --ec2-attributes '{\"KeyName\": \"vockey\"}' \\\n",
    "    --configurations '[{\"Classification\": \"jupyter-s3-conf\", \"Properties\": {\"s3.persistence.enabled\": \"true\", \"s3.persistence.bucket\": \"ethankoz-bucket\"}}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a new cluster, make sure to adjust the security settings to allow for `ssh` access. See `emr_cheatsheet.md` in Week 7 course materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: `ssh` Directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way to work with EMR is to directly `ssh` into it, then work with it just like we did for `EC2` (see previous lab on EC2).\n",
    "\n",
    "Connecting to it:\n",
    "```\n",
    "$ ssh -i <FILE PATH TO vockey.pem> hadoop@<EMR-PUBLIC-ADDRESS>\n",
    "```\n",
    "\n",
    "Uploading a folder called `mystuff` locally -> EMR:\n",
    "```\n",
    "$ scp -i <FILE PATH TO vockey.pem> -r <FILE PATH TO mystuff folder> hadoop@<EMR-PUBLIC-ADDRESS>:/home/hadoop\n",
    "```\n",
    "\n",
    "Downloading a folder called `mystuff` from EMR -> locally:\n",
    "```\n",
    "$ scp -i <FILE PATH TO vockey.pem> -r hadoop@<EMR-PUBLIC-ADDRESS>:/home/hadoop/mystuff .\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading your files in there, you can then run Spark jobs with\n",
    "``` \n",
    "[EMR] spark-submit mystuff/myfile.py\n",
    "```\n",
    "Alternatively if your files are saved on `S3`, then\n",
    "```\n",
    "[EMR] spark-submit s3://ethan-example-bucket/mystuff/myfile.py ethan-example-bucket\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Interactive Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also launch a Jupyter server directly on EMR and work with it interactively.\n",
    "```\n",
    "$ ssh -i \"vockey.pem\" -NL 9443:localhost:9443 hadoop@EMR-PUBLIC-ADDRESS\n",
    "```\n",
    "This forwards the remote connection to your `https://localhost:9443`, and you can log in with username `jovyan`, password `jupyter`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macs30123",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
