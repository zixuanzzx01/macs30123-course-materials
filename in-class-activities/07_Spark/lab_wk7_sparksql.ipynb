{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7 Exercise 2 - EDA & Spark SQL in PySpark\n",
    "\n",
    "First configure our PySpark environment below, and then let's load the data from S3 into our Spark session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load the NOAA Global Historical Climatology Network (GHCN) Daily Weather Data. Documentation of the dataset: https://docs.opendata.aws/noaa-ghcn-pds/readme.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('s3://noaa-ghcn-pds/csv/by_year/2024.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count() # return total count of rows in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that Spark DataFrames are built off of RDDs, we can still use methods like `take` to make small amounts of our data visible as list (we almost will never want to `collect` our data when working with big data, though; **ask**: Why?):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL also includes a `show` method that makes this a bit prettier, though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column types\n",
    "df = df.withColumn(\"DATE\", F.col(\"DATE\").cast(\"int\"))\n",
    "df = df.withColumn(\"DATA_VALUE\", F.col(\"DATA_VALUE\").cast(\"double\"))\n",
    "# convert unit of temperature\n",
    "df = df.withColumn(\"DATA_VALUE\", F.col(\"DATA_VALUE\") / 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following 2 tasks, you can choose to complete using SQL queries or built-in Spark functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 1: \n",
    "Print out the date and maximum temperature from the DataFrame (ELEMENT is \"TMAX\") for each date, and order by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "df.createOrReplaceTempView(\"weather\")\n",
    "query = \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "max_temperatures = spark.sql(query)\n",
    "max_temperatures.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2\n",
    "max_temperatures = # \n",
    "max_temperatures.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 2: Add another condition to filter out temperature lower than 47 degrees Celsius (note the unit of temperature in the data is tenths of degrees Celsius), and change the order to be by max temperature in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "df.createOrReplaceTempView(\"weather\")\n",
    "query = \"\"\"\n",
    "            \n",
    "        \"\"\"\n",
    "max_temperatures = spark.sql(query)\n",
    "max_temperatures.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2\n",
    "max_temperatures = #\n",
    "max_temperatures.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
