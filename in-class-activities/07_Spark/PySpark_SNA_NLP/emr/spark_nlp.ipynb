{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark NLP\n",
    "\n",
    "In this notebook, we will walk through some of the basic functionality of Spark NLP, which can be used to perform more advanced text processing operations than is possible with `pyspark.ml` alone.\n",
    "\n",
    "Note that this notebook is intended to be run on an AWS EMR cluster, using EMR Release 6.2 (With Spark 3.0 installed) (for advanced configuration options, [follow these instructions](https://nlp.johnsnowlabs.com/docs/en/install#emr-support) to bootstrap and configure the cluster via the AWS CLI).\n",
    "\n",
    "-----\n",
    "\n",
    "First, let's load our packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars.packages': 'com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.1', 'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.1\",\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1683728831077_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-38-174.ec2.internal:20888/proxy/application_1683728831077_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-37-113.ec2.internal:8042/node/containerlogs/container_1683728831077_0007_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark-nlp\n",
      "  Using cached https://files.pythonhosted.org/packages/65/19/c439d42f7afd75d6c9c20207db8ee0c95d7c82177b759303c7601120e91a/spark_nlp-4.4.1-py2.py3-none-any.whl\n",
      "Installing collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-4.4.1"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package('spark-nlp', 'https://pypi.org/simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import RegexRule\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, then, let's create a sample DataFrame with some text entries in it that we can work with (drawn from the first several paragraphs of the [University of Chicago's Wikipedia page](https://en.wikipedia.org/wiki/University_of_Chicago)). This data is quite small (purposefully!) so that we can easily see all of the operations that are being performed. Of course, we're running this notebook on a Spark cluster, though, so we can perform these same operations on even the largest DataFrames using this same approach -- whether that is the Amazon Customer Reviews dataset that we've been working with or a text corpus as big as the Common Crawl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------------------------------+\n",
      "| id|                                                                                                text|\n",
      "+---+----------------------------------------------------------------------------------------------------+\n",
      "|  0|The University of Chicago was incorporated as a coeducational institution in 1890 by the American...|\n",
      "|  1|The Hyde Park campus continued the legacy of the original university of the same name, which had ...|\n",
      "+---+----------------------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "sample = [\n",
    "    [0, 'The University of Chicago was incorporated as a coeducational institution in 1890 by the American Baptist Education Society, using $400,000 donated to the ABES to match a $600,000 donation from Baptist oil magnate and philanthropist John D. Rockefeller, and including land donated by Marshall Field. While the Rockefeller donation provided money for academic operations and long-term endowment, it was stipulated that such money could not be used for buildings. The Hyde Park campus was financed by donations from wealthy Chicagoans like Silas B. Cobb who provided the funds for the campus first building, Cobb Lecture Hall, and matched Marshall Fields pledge of $100,000. Other early benefactors included businessmen Charles L. Hutchinson (trustee, treasurer and donor of Hutchinson Commons), Martin A. Ryerson (president of the board of trustees and donor of the Ryerson Physical Laboratory) Adolphus Clay Bartlett and Leon Mandel, who funded the construction of the gymnasium and assembly hall, and George C. Walker of the Walker Museum, a relative of Cobb who encouraged his inaugural donation for facilities.'],\n",
    "    [1, 'The Hyde Park campus continued the legacy of the original university of the same name, which had closed in the 1880s after its campus was foreclosed on. What became known as the Old University of Chicago had been founded by a small group of Baptist educators in 1856 through a land endowment from Senator Stephen A. Douglas. After a fire, it closed in 1886. Alumni from the Old University of Chicago are recognized as alumni of the present University of Chicago. The university depiction on its coat of arms of a phoenix rising from the ashes is a reference to the fire, foreclosure, and demolition of the Old University of Chicago campus. As an homage to this pre-1890 legacy, a single stone from the rubble of the original Douglas Hall on 34th Place was brought to the current Hyde Park location and set into the wall of the Classics Building. These connections have led the dean of the college and University of Chicago and professor of history John Boyer to conclude that the University of Chicago has, a plausible genealogy as a pre–Civil War institution']\n",
    "]\n",
    "\n",
    "data = spark.createDataFrame(sample) \\\n",
    "            .toDF(\"id\", \"text\")\n",
    "\n",
    "data.show(truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll remember from `pyspark.ml` that pipelines can be useful approach for combining various estimators and transformers into a single workflow. Spark NLP extends this idea by introducing so-called \"annotators\" that can perform NLP-related estimation tasks (e.g. things that can be trained through `.fit()`) and transformation tasks (things that can transform one DataFrame into another DataFrame in some way). \n",
    "\n",
    "For instance, below, we transform raw text into a document, transform that document into tokens, and then identify the \"part of speech\" for each token based on a pre-trained POS-tagger. We can chain these transformers and estimators together into a single reproducible pipeline that can then be fit and used to transform data. Note as well that we're using the `Pipeline()` function from `pyspark.ml`, so it's also easy to use plug these annotators into our existing ML workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 3.9 MB\n",
      "[OK!]"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "pos = PerceptronModel.pretrained(\"pos_anc\", 'en')\\\n",
    "        .setInputCols(\"document\", \"token\")\\\n",
    "        .setOutputCol(\"pos\")\n",
    "\n",
    "my_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          tokenizer,\n",
    "          pos\n",
    "      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we transform our data, you can see that we have produced different columns for each of our different steps in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|                text|            document|               token|                 pos|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|The University of...|[[document, 0, 11...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|\n",
      "|  1|The Hyde Park cam...|[[document, 0, 10...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+"
     ]
    }
   ],
   "source": [
    "pipelineModel = my_pipeline.fit(data)\n",
    "\n",
    "# transform data\n",
    "result = pipelineModel.transform(data)\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a closer look at the token-level data, we can see the parts of speech for each of the words in our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+---+---+\n",
      "|id |chunk        |begin|end|pos|\n",
      "+---+-------------+-----+---+---+\n",
      "|0  |The          |0    |2  |DT |\n",
      "|0  |University   |4    |13 |NNP|\n",
      "|0  |of           |15   |16 |IN |\n",
      "|0  |Chicago      |18   |24 |NNP|\n",
      "|0  |was          |26   |28 |VBD|\n",
      "|0  |incorporated |30   |41 |VBN|\n",
      "|0  |as           |43   |44 |IN |\n",
      "|0  |a            |46   |46 |DT |\n",
      "|0  |coeducational|48   |60 |JJ |\n",
      "|0  |institution  |62   |72 |NN |\n",
      "|0  |in           |74   |75 |IN |\n",
      "|0  |1890         |77   |80 |CD |\n",
      "|0  |by           |82   |83 |IN |\n",
      "|0  |the          |85   |87 |DT |\n",
      "|0  |American     |89   |96 |JJ |\n",
      "|0  |Baptist      |98   |104|NNP|\n",
      "|0  |Education    |106  |114|NNP|\n",
      "|0  |Society      |116  |122|NNP|\n",
      "|0  |,            |123  |123|,  |\n",
      "|0  |using        |125  |129|VBG|\n",
      "+---+-------------+-----+---+---+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "pos_df = result.select('id', F.explode(F.arrays_zip('token.result',\n",
    "                                              'token.begin',\n",
    "                                              'token.end', \n",
    "                                              'pos.result', \n",
    "                                          )).alias(\"cols\")) \\\n",
    "               .select(\"id\",\n",
    "                       F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "                       F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "                       F.expr(\"cols['2']\").alias(\"end\"),\n",
    "                       F.expr(\"cols['3']\").alias(\"pos\"),\n",
    "                      )\n",
    "pos_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech tagging is only [one of many available annotators](https://nlp.johnsnowlabs.com/docs/en/annotators) in the Spark NLP ecosystem, though, and you're encouraged to take a look through the documentation. Note, for instance, that there are many pre-trained annotators (using state-of-the-art training procedures) that can be used directly out-of-the-box and inserted into your pipelines.\n",
    "\n",
    "Spark NLP also provides many predefined pipelines that will perform common series of transformations on your data according to pre-trained models (e.g. performing NER with various embedding models, for instance). Here, we'll load in a pre-trained pipeline, which produces NER labels (pre-trained through a series of neural networks) for each of our words to demonstrate how this can work on our mini dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_dl download started this may take some time.\n",
      "Approx size to download 169.4 MB\n",
      "[OK!]"
     ]
    }
   ],
   "source": [
    "pipeline = PretrainedPipeline('explain_document_dl', lang='en')\n",
    "result = pipeline.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can then take a look at the results; not bad for a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+--------+-----+\n",
      "|id |lemma        |stem    |ner  |\n",
      "+---+-------------+--------+-----+\n",
      "|0  |The          |the     |O    |\n",
      "|0  |University   |univers |B-ORG|\n",
      "|0  |of           |of      |I-ORG|\n",
      "|0  |Chicago      |chicago |I-ORG|\n",
      "|0  |be           |wa      |O    |\n",
      "|0  |incorporate  |incorpor|O    |\n",
      "|0  |as           |a       |O    |\n",
      "|0  |a            |a       |O    |\n",
      "|0  |coeducational|coeduc  |O    |\n",
      "|0  |institution  |institut|O    |\n",
      "|0  |in           |in      |O    |\n",
      "|0  |1890         |1890    |O    |\n",
      "|0  |by           |by      |O    |\n",
      "|0  |the          |the     |O    |\n",
      "|0  |American     |american|B-ORG|\n",
      "|0  |Baptist      |baptist |I-ORG|\n",
      "|0  |Education    |educ    |I-ORG|\n",
      "|0  |Society      |societi |I-ORG|\n",
      "|0  |,            |,       |O    |\n",
      "|0  |use          |us      |O    |\n",
      "+---+-------------+--------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "ner_df = result.select('id', F.explode(F.arrays_zip('lemma.result',\n",
    "                                 'stem.result', \n",
    "                                 'ner.result'\n",
    "                                         )).alias(\"cols\")) \\\n",
    "               .select(\"id\",\n",
    "                       F.expr(\"cols['0']\").alias(\"lemma\"),\n",
    "                       F.expr(\"cols['1']\").alias(\"stem\"),\n",
    "                       F.expr(\"cols['2']\").alias(\"ner\"),\n",
    "                      )\n",
    "\n",
    "ner_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   ner|count|\n",
      "+------+-----+\n",
      "|B-MISC|    3|\n",
      "| I-ORG|   24|\n",
      "| I-PER|   12|\n",
      "| I-LOC|    7|\n",
      "| B-PER|   16|\n",
      "|     O|  305|\n",
      "| B-ORG|   14|\n",
      "| B-LOC|    8|\n",
      "+------+-----+"
     ]
    }
   ],
   "source": [
    "ner_df.groupBy('ner') \\\n",
    "      .count() \\\n",
    "      .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "\n",
    "That's all we'll cover with regard to Spark NLP, but you're encouraged to play around with it further (perhaps [training your own NER model on GPUs](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/blogposts/3.NER_with_BERT.ipynb)!) and read the [excellent documentation](https://nlp.johnsnowlabs.com/docs/en/concepts) and [tutorials](https://nlp.johnsnowlabs.com/classify_documents) in more depth.\n",
    "\n",
    "## Activity\n",
    "\n",
    "Suppose you are engineering features that correspond to each row in the provided sample dataframe `data` (i.e. each line of text) to use as predictors in a machine learning model, based on the results of your Spark NLP annotations. Specifically, you want to:\n",
    "\n",
    "1. Engineer a feature that computes the number of adjectives (JJ) in each row of the DataFrame.\n",
    "2. Engineer another feature that computes the number of organizations mentioned in each row of the DataFrame (as counted by the B-ORG NER label -- i.e. the beginning of an organization name).\n",
    "3. Merge these counts back into the original DataFrame `data` as columns in the DataFrame (i.e. an `adj_count` column and a `org_count` column) so that you can use them to train your ML model. Note that you can perform inner joins on DataFrames in Spark much like you can in Pandas via `df1.join(df2, on='shared_col_name')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
