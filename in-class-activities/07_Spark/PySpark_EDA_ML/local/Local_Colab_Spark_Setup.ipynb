{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpaNU3ETh0Qc"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1q6hcMHXWR1Cy-tJV2NuKtn603PA-u_PK\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Setting up PySpark in a Colab Notebook\n",
        "\n",
        "You can run Spark both locally and on a cluster. Here, I'll demonstrate how you can set up Spark to run in a Colab notebook for debugging purposes.\n",
        "\n",
        "You can also set up Spark locally in a similar way if you want to take advantage of multiple CPU cores (and/or GPU) on your laptop (the setup will vary slightly, though, depending on your operating system and you'll need to figure out these specifics on your own; however, this setup does work in WSL for me if I run the follow bash script in my terminal window using `sudo`). This being said, this local option should be for testing purposes on sample datasets only. If you want to run big PySpark jobs, you will want to run these in an EMR notebook (with an EMR cluster as your backend) or on the Midway Cluster.\n",
        "\n",
        "First, we need to install Spark and PySpark, by running the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8f1D7wfaCgF",
        "outputId": "7e33421c-db25-44a1-d40e-68705afc8126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.0/317.0 MB 3.3 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "apt-get update -qq\n",
        "apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "wget -q \"https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\" > /dev/null\n",
        "tar -xvf spark-3.1.1-bin-hadoop2.7.tgz > /dev/null\n",
        "\n",
        "pip install pyspark findspark --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHjZLeId0nvR"
      },
      "source": [
        "OK, now that we have Spark, we need to set a path to it, so PySpark knows where to find it. We do this using the `os` Python library below.\n",
        "\n",
        "On my machine (WSL, Ubuntu 20.04), where I unpacked Spark in my home directory, this can be achieved with:\n",
        "```\n",
        "os.environ[\"SPARK_HOME\"] = \"/home/USERNAME/spark-3.1.1-bin-hadoop2.7\"\n",
        "```\n",
        "\n",
        "In Colab, it is automatically downloaded to the `/content` directory, so we indicate that as its location here. Then, we run `findspark` to find Spark for us on the machine, and finally start up a SparkSession running on all available cores (`local[4]` means your code will run on 4 threads locally, `local[*]` means that your code will run as many threads as there are logical cores on your machine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CljIupW0aE06"
      },
      "outputs": [],
      "source": [
        "# Set path to Spark\n",
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
        "\n",
        "# Find Spark so that we can access session within our notebook\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Start SparkSession on all available cores\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNTQOBLthDrC"
      },
      "source": [
        "Now that we've installed everything and set up our paths correctly, we can run (small) Spark jobs both in Colab notebooks and locally (for bigger jobs, you will want to run these jobs on an EMR cluster, though. Remember, for instance, that Google only allocates us one CPU core and up to one GPU for free)!\n",
        "\n",
        "Let's make sure our setup is working by doing couple of simple things with the pyspark.sql package on the Amazon Customer Review Sample Dataset we looked at last week with `mrjob`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fbXWBQfSAX8q",
        "outputId": "0aa2c5dc-0fa1-4b17-9448-d23f1d4e95f0"
      },
      "outputs": [],
      "source": [
        "! pip install wget\n",
        "import wget\n",
        "\n",
        "wget.download('https://css-uchicago.s3.us-east-1.amazonaws.com/sample_us.tsv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KrXWEMxjeFx1"
      },
      "outputs": [],
      "source": [
        "# Read TSV file\n",
        "data = spark.read.csv('sample_us.tsv',\n",
        "                      sep=\"\\t\",\n",
        "                      header=True,\n",
        "                      inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qvOOIYqeWw9",
        "outputId": "e9c0ba2e-cc5b-4c9b-db1f-ebb734ac598c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- marketplace: string (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- review_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- product_parent: integer (nullable = true)\n",
            " |-- product_title: string (nullable = true)\n",
            " |-- product_category: string (nullable = true)\n",
            " |-- star_rating: integer (nullable = true)\n",
            " |-- helpful_votes: integer (nullable = true)\n",
            " |-- total_votes: integer (nullable = true)\n",
            " |-- vine: string (nullable = true)\n",
            " |-- verified_purchase: string (nullable = true)\n",
            " |-- review_headline: string (nullable = true)\n",
            " |-- review_body: string (nullable = true)\n",
            " |-- review_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngb25JINcUNr",
        "outputId": "9559407b-ea60-4cd7-d89c-9e2a29438c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+----------------+\n",
            "|star_rating|sum(total_votes)|\n",
            "+-----------+----------------+\n",
            "|          5|              13|\n",
            "|          4|               3|\n",
            "|          3|               8|\n",
            "|          2|               2|\n",
            "|          1|               8|\n",
            "+-----------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data.groupBy('star_rating') \\\n",
        "     .sum('total_votes') \\\n",
        "     .sort('star_rating', ascending=False) \\\n",
        "     .show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Local/Colab Spark Setup",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
